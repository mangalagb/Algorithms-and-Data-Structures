Insertion Sort
------------------
	1) It is very simple.
	2) It is very efficient for small data sets.
	3) It is stable; i.e., it does not change the relative order of elements with equal keys.
	4) In-place; i.e., only requires a constant amount O(1) of additional memory space.
	
	The best case input is an array that is already sorted. In this case insertion sort has a linear running time (i.e., Θ(n)).
	
	The simplest worst case input is an array sorted in reverse order.
	The set of all worst case inputs consists of all arrays where each element is the smallest or second-smallest of the elements before it.
	In these cases every iteration of the inner loop will scan and shift the entire sorted subsection of the array before inserting the next element. 
	This gives insertion sort a quadratic running time (i.e., O(n2)).
	
	The average case is also quadratic, which makes insertion sort impractical for sorting large arrays.
	
	However, insertion sort is one of the fastest algorithms for sorting very small arrays(around 10), even faster than quicksort.

_______________________________________________________________________________________________________________________________________________________________
Selection Sort
------------------

  In selection sort algorithm, we search for the lowest element and arrange it to the proper location.
  We swap the current element with the next lowest number.
  
  Selection sort is good to use for small number of input elements
  
  Best Case        :    O(n^2)
  Average Case   :    O(n^2)
  Worst Case      :    O(n^2)
  
  Among simple average-case Θ(n2) algorithms, selection sort almost always outperforms bubble sort and gnome sort, but is generally outperformed by insertion sort.
  
  Insertion sort's advantage is that it only scans as many elements as it needs in order to place the k + 1st element, 
  while selection sort must scan all remaining elements to find the k + 1st element.
  
  While selection sort is preferable to insertion sort in terms of number of writes (Θ(n) swaps versus Ο(n2) swaps),
  it almost always far exceeds (and never beats) the number of writes that cycle sort makes, as cycle sort is theoretically optimal in the number of writes.
  This can be important if writes are significantly more expensive than reads, such as with EEPROM or Flash memory, 
  where every write lessens the lifespan of the memory.
  
_______________________________________________________________________________________________________________________________________________________________

Bubble Sort
------------------
  Bubble sort has worst-case and average complexity both О(n2), where n is the number of items being sorted. 
  There exist many sorting algorithms with substantially better worst-case or average complexity of O(n log n). 
  Even other О(n2) sorting algorithms, such as insertion sort, tend to have better performance than bubble sort. 
  Therefore, bubble sort is not a practical sorting algorithm when n is large.
  
  The only significant advantage that bubble sort has over most other implementations, even quicksort, but not insertion sort,
  is that the ability to detect that the list is sorted efficiently is built into the algorithm.
  When the list is already sorted (best-case), the complexity of bubble sort is only O(n).
  
  However, not only does insertion sort have this mechanism too, but it also performs
  better on a list that is substantially sorted (having a small number of inversions).
_______________________________________________________________________________________________________________________________________________________________

Quick Sort
------------------
   Uses divide-and-conquer.
   The basic step of sorting an array are as follows:
	1) Select a pivot, normally the middle one
	2) From both ends, swap elements and make left elements less than pivot and all right greater than pivot.
	3) Recursively sort left part and right part
	
    Average time complexity is O(n log(n)).
    Worst case time complexity is O(n^2)
    
    Worst Case: The worst case occurs when the partition process always picks greatest or smallest element as pivot. 
    If we consider above partition strategy where last element is always picked as pivot, the worst case would occur 
    when the array is already sorted in increasing or decreasing order. Following is recurrence for worst case.
    
    The best case occurs when the partition process always picks the middle element as pivot.
    
    Although the worst case time complexity of QuickSort is O(n2) which is more than many other sorting algorithms like Merge Sort and Heap Sort, 
    QuickSort is faster in practice, because its inner loop can be efficiently implemented on most architectures, and in most real-world data.
    QuickSort can be implemented in different ways by changing the choice of pivot, so that the worst case rarely occurs for a given type of data. 
    However, merge sort is generally considered better when data is huge and stored in external storage.
	
	Always pick first element as pivot.
	Always pick last element as pivot (implemented below)
	Pick a random element as pivot.
	Pick median as pivot.
_______________________________________________________________________________________________________________________________________________________________
	
Merge Sort
------------------
  1) Merge sort is used when the data structure doesn’t support random access, since it works with pure sequential access (forward iterators, rather than random access iterators). 
  	It’s also widely used for external sorting, where random access can be very, very expensive compared to sequential access.
	For example, When sorting a file which doesn’t fit into memory, you might break it into chunks which fit into memory, 
	sort these using independently, writing each out to a file, then merge sort the generated files.
	
	2) You can use merge sort when you need a stable sort. It’s very important feature of merge sort.
	
	3) Mergesort is quicker when dealing with linked lists. This is because pointers can easily be changed when merging lists.
	  It only requires one pass (O(n)) through the list.
	  
	4) If there is a lot of parallelization occurs then parallelizing Mergesort is simpler than other sort algorithms.